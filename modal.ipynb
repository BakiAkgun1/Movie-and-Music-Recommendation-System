{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering # type: ignore\n",
    "from sklearn.mixture import GaussianMixture # type: ignore\n",
    "from sklearn.cluster import DBSCAN # type: ignore\n",
    "from sklearn.metrics import silhouette_score # type: ignore\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "import pandas as pd # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       culture clash future space war space colony so...\n",
       "1       ocean drug abuse exotic island east india trad...\n",
       "2       spy based novel secret agent sequel mi action ...\n",
       "3       dc comics crime fighter terrorist secret ident...\n",
       "4       based novel mars medallion space travel prince...\n",
       "                              ...                        \n",
       "4798    united states umexico barrier legs arms paper ...\n",
       "4799    unknown comedy romance newlywed couple honeymo...\n",
       "4800    date love first sight narration investigation ...\n",
       "4801    unknown drama ambitious new york attorney sam ...\n",
       "4802    obsession camcorder crush dream girl documenta...\n",
       "Name: combined_features, Length: 4801, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(\"clean_data.csv\")\n",
    "data =  data.dropna(subset=['keywords', 'genres','overview'])\n",
    "\n",
    "data['combined_features'] = data['keywords'] + ' ' + data['genres'] + ' ' + data['overview']\n",
    "data['combined_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidfVectorizer ile metni sayısal veriye dönüştürme\n",
    "tfidf = TfidfVectorizer(stop_words ='english')\n",
    "tfidf_matrix = tfidf.fit_transform(data['combined_features'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hieararchical Clustering \n",
    "hier_clustering = AgglomerativeClustering(n_clusters= 10)\n",
    "hier_labels = hier_clustering.fit_predict(tfidf_matrix.toarray())\n",
    "silhouette_hier= silhouette_score(tfidf_matrix, hier_labels)\n",
    "print(f\"Hierarchical Clustering Silhoutte Score: {silhouette_hier}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBSCAM Silhoutte Score: -1\n"
     ]
    }
   ],
   "source": [
    "#DBSCAN\n",
    "dbscan = DBSCAN(eps = 0.5, min_samples=5)\n",
    "dbscan_labels = dbscan.fit_predict(tfidf_matrix.toarray())\n",
    "\n",
    "if len(set(dbscan_labels)) > 1 and -1 not in dbscan_labels:\n",
    "    silhoutte_dbscan = silhouette_score(tfidf_matrix, dbscan_labels)\n",
    "else:\n",
    "    silhoutte_dbscan = -1 #dbscan başarısızz  ise  -1 yazalım\n",
    "print(f\"DBSCAN Silhoutte Score: {silhoutte_dbscan}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GM Silhoutte Score: -0.010837713742997463\n"
     ]
    }
   ],
   "source": [
    "#Gaussian Mixture Model(GMM\n",
    "\n",
    "from sklearn.decomposition import PCA # type: ignore\n",
    "pca = PCA(n_components=100)\n",
    "tfidf_matrix_reduced = pca.fit_transform(tfidf_matrix.toarray())\n",
    "\n",
    "gmm = GaussianMixture(n_components=10, random_state=42)\n",
    "gmm_labels= gmm.fit_predict(tfidf_matrix_reduced)\n",
    "silhouette_gmm = silhouette_score(tfidf_matrix_reduced, gmm_labels)\n",
    "print(f\"GM Silhoutte Score: {silhouette_gmm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results\n",
    "clustering_methods = ['Hierarchical','DBSCAN','GMM']\n",
    "silhouette_scores = [silhouette_hier, silhoutte_dbscan, silhouette_gmm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_method_index = np.argmax(silhouette_scores)\n",
    "best_method = clustering_methods[best_method_index]\n",
    "best_score = silhouette_scores[best_method_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En iyi kümeleme yöntemi: Hierarchical\n",
      "En yüksek Silhouette Skoru: -0.0030384601312127874\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"En iyi kümeleme yöntemi: {best_method}\")\n",
    "print(f\"En yüksek Silhouette Skoru: {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import fpgrowth, association_rules,apriori # type: ignore\n",
    "from sklearn.feature_extraction.text import CountVectorizer # type: ignore\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FPGrowth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Baki Akgun\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(tokenizer = lambda x: x.split(' '), binary=True)\n",
    "X = vectorizer.fit_transform(data['combined_features'])\n",
    "features  = vectorizer.get_feature_names_out()\n",
    "\n",
    "df_features = pd.DataFrame(X.toarray(), columns = features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Baki Akgun\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sık kullanılan Ögeler:       support                     itemsets\n",
      "0    0.249948                     (action)\n",
      "1    0.173089                  (adventure)\n",
      "2    0.113726                    (science)\n",
      "3    0.112477                    (fiction)\n",
      "4    0.090606                    (fantasy)\n",
      "..        ...                          ...\n",
      "535  0.047282          (independent, film)\n",
      "536  0.035618         (independent, drama)\n",
      "537  0.024995        (independent, comedy)\n",
      "538  0.034160   (independent, drama, film)\n",
      "539  0.024162  (independent, comedy, film)\n",
      "\n",
      "[540 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "frequent_itemsets = fpgrowth(df_features, min_support = 0.02, use_colnames = True)\n",
    "rules = association_rules(frequent_itemsets, metric = \"lift\", min_threshold = 1)\n",
    "print(f\"Sık kullanılan Ögeler: {frequent_itemsets}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Birliktelik Kuralları:              antecedents            consequents   support  confidence  \\\n",
      "0             (thriller)               (action)  0.112268    0.424076   \n",
      "1               (action)             (thriller)  0.112268    0.449167   \n",
      "2      (thriller, drama)               (action)  0.034368    0.295699   \n",
      "3        (drama, action)             (thriller)  0.034368    0.460894   \n",
      "4             (thriller)        (drama, action)  0.034368    0.129819   \n",
      "..                   ...                    ...       ...         ...   \n",
      "649  (independent, film)               (comedy)  0.024162    0.511013   \n",
      "650       (comedy, film)          (independent)  0.024162    0.588832   \n",
      "651        (independent)         (comedy, film)  0.024162    0.489451   \n",
      "652             (comedy)    (independent, film)  0.024162    0.067052   \n",
      "653               (film)  (independent, comedy)  0.024162    0.240664   \n",
      "\n",
      "          lift  \n",
      "0     1.696656  \n",
      "1     1.696656  \n",
      "2     1.183042  \n",
      "3     1.740953  \n",
      "4     1.740953  \n",
      "..         ...  \n",
      "649   1.418136  \n",
      "650  11.928206  \n",
      "651  11.928206  \n",
      "652   1.418136  \n",
      "653   9.628562  \n",
      "\n",
      "[654 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n  Birliktelik Kuralları: {rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Baki Akgun\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data['combined_features'] = data['combined_features'].apply(lambda x: x.split())\n",
    "vectorizer = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False, binary=True)\n",
    "X = vectorizer.fit_transform(data['combined_features'])\n",
    "df_features = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [culture, clash, future, space, war, space, co...\n",
       "1    [ocean, drug, abuse, exotic, island, east, ind...\n",
       "2    [spy, based, novel, secret, agent, sequel, mi,...\n",
       "3    [dc, comics, crime, fighter, terrorist, secret...\n",
       "4    [based, novel, mars, medallion, space, travel,...\n",
       "Name: combined_features, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['combined_features'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Baki Akgun\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sık  ullanılan Ögeler:       support                               itemsets\n",
      "0    0.249948                               (action)\n",
      "1    0.173089                            (adventure)\n",
      "2    0.022704                                  (age)\n",
      "3    0.034368                                (agent)\n",
      "4    0.027286                                (along)\n",
      "..        ...                                    ...\n",
      "535  0.020204                   (true, drama, story)\n",
      "536  0.042908           (science, fiction, thriller)\n",
      "537  0.027494              (science, fiction, world)\n",
      "538  0.030619  (science, fiction, adventure, action)\n",
      "539  0.025828   (science, fiction, action, thriller)\n",
      "\n",
      "[540 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "frequent_itemsets = apriori(df_features, min_support= 0.02, use_colnames= True)\n",
    "\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold= 1)\n",
    "\n",
    "print(f\"Sık  ullanılan Ögeler: {frequent_itemsets}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Association Rules:             antecedents                   consequents   support  confidence  \\\n",
      "0           (adventure)                      (action)  0.099354    0.574007   \n",
      "1              (action)                   (adventure)  0.099354    0.397500   \n",
      "2               (agent)                      (action)  0.022495    0.654545   \n",
      "3              (action)                       (agent)  0.022495    0.090000   \n",
      "4              (action)                       (based)  0.024162    0.096667   \n",
      "..                  ...                           ...       ...         ...   \n",
      "649  (thriller, action)            (science, fiction)  0.025828    0.230056   \n",
      "650           (science)   (thriller, fiction, action)  0.025828    0.227106   \n",
      "651           (fiction)   (science, action, thriller)  0.025828    0.229630   \n",
      "652            (action)  (science, fiction, thriller)  0.025828    0.103333   \n",
      "653          (thriller)    (science, fiction, action)  0.025828    0.097561   \n",
      "\n",
      "         lift  \n",
      "0    2.296507  \n",
      "1    2.296507  \n",
      "2    2.618727  \n",
      "3    2.618727  \n",
      "4    1.097155  \n",
      "..        ...  \n",
      "649  2.076123  \n",
      "650  8.722696  \n",
      "651  8.890741  \n",
      "652  2.408269  \n",
      "653  1.678818  \n",
      "\n",
      "[654 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n Association Rules: {rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spotify Modal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veriyi yükleme\n",
    "df = pd.read_csv(\"spotify.csv\")\n",
    "\n",
    "# Numeric sütunları seçiyoruz\n",
    "numeric_features = ['instrumentalness', 'energy', 'danceability', 'speechiness', 'liveness', 'valence']\n",
    "X = df[numeric_features]  # Bağımsız değişkenler\n",
    "y = df['tempo']           # Bağımlı değişken (örneğin 'tempo'yu tahmin etmeye çalışıyoruz)\n",
    "\n",
    "# Veriyi eğitim ve test setlerine bölme\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Regresyon modelleri tanımlama\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Random Forest\": RandomForestRegressor(),\n",
    "    \"XGBoost\": XGBRegressor(objective='reg:squarederror')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Önerileri:\n",
      "['Nós vamos invadir sua praia', 'Bring Me To Life', 'Bring Me To Life', 'Ego Death', 'Em Respeito Ao Vício', 'Ego Death', 'Sparkle - Original Version', 'Comedy', 'Em Respeito Ao Vício', 'Comedy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Baki Akgun\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but NearestNeighbors was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Veriyi yükleme\n",
    "df = pd.read_csv(\"spotify.csv\")\n",
    "\n",
    "# Sayısal özellikleri al\n",
    "numeric_features = df[['instrumentalness', 'energy']]\n",
    "\n",
    "# 1. KNN ile Benzerlik Hesaplama\n",
    "def knn_recommendations(song_name, n_neighbors=10):\n",
    "    # KNN modeli oluşturma\n",
    "    knn = NearestNeighbors(n_neighbors=n_neighbors, metric='cosine').fit(numeric_features)\n",
    "    \n",
    "    # Seçilen şarkıyı bulma\n",
    "    selected_song_index = df[df['track_name'] == song_name].index[0]\n",
    "    selected_song_features = numeric_features.iloc[selected_song_index].values.reshape(1, -1)\n",
    "\n",
    "    # Benzer şarkıları bulma\n",
    "    distances, indices = knn.kneighbors(selected_song_features)\n",
    "    \n",
    "    # Önerilen şarkıları döndür\n",
    "    recommended_songs = df.iloc[indices[0]]['track_name'].tolist()\n",
    "    return recommended_songs\n",
    "\n",
    "# Örnek şarkı ismi\n",
    "sample_song = df['track_name'].iloc[0]  # İlk şarkıyı al\n",
    "print(\"KNN Önerileri:\")\n",
    "print(knn_recommendations(sample_song))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   track_name  Cluster\n",
      "0                      Comedy       -1\n",
      "1            Ghost - Acoustic        0\n",
      "2              To Begin Again        0\n",
      "3  Can't Help Falling In Love        0\n",
      "4                     Hold On        0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# DBSCAN modeli\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "clusters = dbscan.fit_predict(scaled_features)\n",
    "\n",
    "# Küme bilgilerini ekleyelim\n",
    "df['Cluster'] = clusters\n",
    "print(df[['track_name', 'Cluster']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agglomerative Clustering Silhouette Skoru (Sampled): 0.1608\n",
      "                   track_name  Agglomerative_Cluster\n",
      "113186          No Other Name                      0\n",
      "42819          Failed Organum                      1\n",
      "59311   Save the Trees, Pt. 1                      0\n",
      "91368         Merry Christmas                      0\n",
      "61000                   月の大きさ                      0\n"
     ]
    }
   ],
   "source": [
    "# Sample 10% of the data\n",
    "df_sampled = df.sample(frac=0.1, random_state=42)\n",
    "scaled_sampled_features = scaled_features[df_sampled.index]\n",
    "\n",
    "agglo = AgglomerativeClustering(n_clusters=5)\n",
    "agglo_clusters = agglo.fit_predict(scaled_sampled_features)\n",
    "\n",
    "# Silhouette score for sampled data\n",
    "agglo_silhouette = silhouette_score(scaled_sampled_features, agglo_clusters)\n",
    "print(f\"Agglomerative Clustering Silhouette Skoru (Sampled): {agglo_silhouette:.4f}\")\n",
    "\n",
    "# Add cluster information back to the original DataFrame\n",
    "df_sampled['Agglomerative_Cluster'] = agglo_clusters\n",
    "print(df_sampled[['track_name', 'Agglomerative_Cluster']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"K-Means Silhouette Skoru: {kmeans_silhouette:.4f}\")\n",
    "print(f\"DBSCAN Silhouette Skoru: {dbscan_silhouette:.4f}\")\n",
    "print(f\"Agglomerative Clustering Silhouette Skoru: {agglo_silhouette:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KMeans Kümeleme Sonuçları:\n",
      "Silhouette Skoru: 0.1844\n",
      "                   track_name  KMeans_Cluster\n",
      "0                      Comedy               2\n",
      "1            Ghost - Acoustic               1\n",
      "2              To Begin Again               1\n",
      "3  Can't Help Falling In Love               1\n",
      "4                     Hold On               1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def kmeans_clustering(n_clusters=5):\n",
    "    # Veriyi ölçeklendir\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(numeric_features)\n",
    "\n",
    "    # KMeans modeli oluştur\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(scaled_features)\n",
    "\n",
    "    # Silhouette skoru hesapla\n",
    "    silhouette_avg = silhouette_score(scaled_features, clusters)\n",
    "    print(f\"Silhouette Skoru: {silhouette_avg:.4f}\")\n",
    "\n",
    "    # Küme bilgilerini DataFrame'e ekle\n",
    "    df['KMeans_Cluster'] = clusters\n",
    "    return df[['track_name', 'KMeans_Cluster']]\n",
    "\n",
    "print(\"\\nKMeans Kümeleme Sonuçları:\")\n",
    "print(kmeans_clustering().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verinin %5'lik bir örneğini alalım\n",
    "df_sampled = df.sample(frac=0.05, random_state=42)\n",
    "\n",
    "# Seçtiğiniz özellikleri kullanarak devam edelim\n",
    "selected_features_sampled = df_sampled[['tempo', 'instrumentalness', 'energy']]\n",
    "\n",
    "# Veriyi ölçekleyelim (Scaling)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_sampled_features = scaler.fit_transform(selected_features_sampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# PCA ile boyut indirgeme (Örneğin 3 bileşene düşürelim)\n",
    "pca = PCA(n_components=3)\n",
    "reduced_sampled_features = pca.fit_transform(scaled_sampled_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   track_name  KMeans_Cluster\n",
      "113186          No Other Name               0\n",
      "42819          Failed Organum               4\n",
      "59311   Save the Trees, Pt. 1               2\n",
      "91368         Merry Christmas               1\n",
      "61000                   月の大きさ               2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "# MiniBatchKMeans ile kümelenme\n",
    "kmeans = MiniBatchKMeans(n_clusters=5, random_state=42, batch_size=1000)\n",
    "kmeans_clusters = kmeans.fit_predict(reduced_sampled_features)\n",
    "\n",
    "# Sonuçları DataFrame'e ekleyelim\n",
    "df_sampled['KMeans_Cluster'] = kmeans_clusters\n",
    "print(df_sampled[['track_name', 'KMeans_Cluster']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniBatchKMeans Silhouette Skoru: 0.3632\n",
      "                   track_name  KMeans_Cluster\n",
      "113186          No Other Name               0\n",
      "42819          Failed Organum               4\n",
      "59311   Save the Trees, Pt. 1               2\n",
      "91368         Merry Christmas               1\n",
      "61000                   月の大きさ               2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# MiniBatchKMeans ile kümelenme\n",
    "kmeans = MiniBatchKMeans(n_clusters=5, random_state=42, batch_size=1000)\n",
    "kmeans_clusters = kmeans.fit_predict(reduced_sampled_features)\n",
    "\n",
    "# Silhouette skorunu hesaplayalım\n",
    "silhouette_avg = silhouette_score(reduced_sampled_features, kmeans_clusters)\n",
    "print(f\"MiniBatchKMeans Silhouette Skoru: {silhouette_avg:.4f}\")\n",
    "\n",
    "# Sonuçları DataFrame'e ekleyelim\n",
    "df_sampled['KMeans_Cluster'] = kmeans_clusters\n",
    "print(df_sampled[['track_name', 'KMeans_Cluster']].head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
